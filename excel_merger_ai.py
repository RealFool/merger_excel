#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Excelå¤šè¡¨åˆå¹¶å·¥å…· - AIæ™ºèƒ½åŒ¹é…è¡¨å¤´
ä½¿ç”¨DeepSeek LLMè¯†åˆ«è¯­ä¹‰ç›¸åŒä½†è¡¨è¿°ä¸åŒçš„å­—æ®µï¼Œå®ç°å¼‚æ„è¡¨æ ¼æ™ºèƒ½å¯¹é½åˆå¹¶

åŠŸèƒ½ç‰¹ç‚¹ï¼š
1. æ”¯æŒå¤šä¸ªExcelæ–‡ä»¶æ‰¹é‡åˆå¹¶
2. AIæ™ºèƒ½è¯†åˆ«ç›¸ä¼¼å­—æ®µåï¼ˆå¦‚ï¼šè®¢å•IDã€ID_è®¢å•ã€è®¢å•ç¼–å·ã€é”€å”®å•å·ï¼‰
3. è‡ªåŠ¨å¯¹é½è¡¨å¤´ï¼Œå¤„ç†å¼‚æ„è¡¨æ ¼
4. æ”¯æŒæ•°æ®å»é‡å’Œå†²çªå¤„ç†
5. ç”Ÿæˆåˆå¹¶æŠ¥å‘Š

ä½œè€…ï¼šAI Assistant
æ—¥æœŸï¼š2025-01-29
"""

import os
import json
import pandas as pd
import requests
from typing import List, Dict, Tuple, Optional
from pathlib import Path
import logging
from datetime import datetime
import hashlib
from collections import defaultdict
import re

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('excel_merger.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class DeepSeekAPI:
    """DeepSeek APIè°ƒç”¨ç±»"""
    
    def __init__(self, api_key: str = None, base_url: str = "https://api.deepseek.com"):
        self.api_key = api_key or "sk-your-deepseek-api-key"  # è¯·æ›¿æ¢ä¸ºå®é™…çš„API Key
        self.base_url = base_url
        self.headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
    
    def call_api(self, messages: List[Dict], model: str = "deepseek-chat") -> str:
        """è°ƒç”¨DeepSeek API"""
        try:
            payload = {
                "model": model,
                "messages": messages,
                "temperature": 0.1,
                "max_tokens": 2000
            }
            
            response = requests.post(
                f"{self.base_url}/v1/chat/completions",
                headers=self.headers,
                json=payload,
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                return result['choices'][0]['message']['content']
            else:
                logger.error(f"APIè°ƒç”¨å¤±è´¥: {response.status_code} - {response.text}")
                return ""
                
        except Exception as e:
            logger.error(f"APIè°ƒç”¨å¼‚å¸¸: {e}")
            return ""

class FieldMatcher:
    """å­—æ®µåŒ¹é…å™¨ - ä½¿ç”¨AIè¯†åˆ«ç›¸ä¼¼å­—æ®µ"""
    
    def __init__(self, deepseek_api: DeepSeekAPI):
        self.api = deepseek_api
        self.field_mapping_cache = {}
    
    def analyze_field_similarity(self, field_groups: List[List[str]]) -> Dict[str, List[str]]:
        """åˆ†æå­—æ®µç›¸ä¼¼æ€§ï¼Œè¿”å›å­—æ®µæ˜ å°„å…³ç³»"""
        
        # æ„å»ºæ‰€æœ‰å­—æ®µåˆ—è¡¨
        all_fields = []
        for group in field_groups:
            all_fields.extend(group)
        
        # å»é‡
        unique_fields = list(set(all_fields))
        
        if len(unique_fields) <= 1:
            return {unique_fields[0]: unique_fields} if unique_fields else {}
        
        # æ„å»ºAIæç¤ºè¯
        prompt = self._build_field_analysis_prompt(unique_fields)
        
        messages = [
            {
                "role": "system",
                "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ•°æ®åˆ†æå¸ˆï¼Œæ“…é•¿è¯†åˆ«è¡¨æ ¼ä¸­è¯­ä¹‰ç›¸åŒä½†è¡¨è¿°ä¸åŒçš„å­—æ®µåã€‚è¯·ä»”ç»†åˆ†æå­—æ®µåçš„è¯­ä¹‰å«ä¹‰ï¼Œå°†ç›¸åŒå«ä¹‰çš„å­—æ®µå½’ä¸ºä¸€ç»„ã€‚"
            },
            {
                "role": "user",
                "content": prompt
            }
        ]
        
        # è°ƒç”¨API
        response = self.api.call_api(messages)
        
        # è§£æå“åº”
        field_mapping = self._parse_field_mapping_response(response, unique_fields)
        
        logger.info(f"å­—æ®µæ˜ å°„ç»“æœ: {field_mapping}")
        return field_mapping
    
    def _build_field_analysis_prompt(self, fields: List[str]) -> str:
        """æ„å»ºå­—æ®µåˆ†ææç¤ºè¯"""
        fields_text = "\n".join([f"{i+1}. {field}" for i, field in enumerate(fields)])
        
        prompt = f"""
è¯·åˆ†æä»¥ä¸‹å­—æ®µåï¼Œè¯†åˆ«è¯­ä¹‰ç›¸åŒä½†è¡¨è¿°ä¸åŒçš„å­—æ®µï¼Œå¹¶å°†å®ƒä»¬å½’ä¸ºä¸€ç»„ã€‚

å­—æ®µåˆ—è¡¨ï¼š
{fields_text}

åˆ†æè¦æ±‚ï¼š
1. è¯†åˆ«è¡¨ç¤ºç›¸åŒä¸šåŠ¡å«ä¹‰çš„å­—æ®µï¼ˆå¦‚ï¼šè®¢å•IDã€ID_è®¢å•ã€è®¢å•ç¼–å·ã€é”€å”®å•å·éƒ½è¡¨ç¤ºè®¢å•æ ‡è¯†ï¼‰
2. è€ƒè™‘å¸¸è§çš„ä¸šåŠ¡å­—æ®µï¼šå®¢æˆ·ä¿¡æ¯ã€è®¢å•ä¿¡æ¯ã€äº§å“ä¿¡æ¯ã€é‡‘é¢ã€æ—¶é—´ç­‰
3. å¿½ç•¥å¤§å°å†™ã€ä¸‹åˆ’çº¿ã€ç©ºæ ¼ç­‰æ ¼å¼å·®å¼‚
4. è€ƒè™‘ä¸­è‹±æ–‡æ··ç”¨ã€ç®€å†™ã€å…¨ç§°ç­‰æƒ…å†µ

è¯·ä»¥JSONæ ¼å¼è¿”å›ç»“æœï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{{
  "group_1": ["å­—æ®µ1", "å­—æ®µ2", "å­—æ®µ3"],
  "group_2": ["å­—æ®µ4", "å­—æ®µ5"],
  "group_3": ["å­—æ®µ6"]
}}

å…¶ä¸­æ¯ä¸ªgroupä»£è¡¨ä¸€ç»„è¯­ä¹‰ç›¸åŒçš„å­—æ®µï¼Œgroupçš„å‘½åè¯·ä½¿ç”¨æœ€é€šç”¨çš„å­—æ®µåã€‚
"""
        return prompt
    
    def _parse_field_mapping_response(self, response: str, original_fields: List[str]) -> Dict[str, List[str]]:
        """è§£æAIå“åº”ï¼Œæå–å­—æ®µæ˜ å°„å…³ç³»"""
        try:
            # å°è¯•æå–JSON
            json_match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', response, re.DOTALL)
            if json_match:
                json_str = json_match.group()
                mapping = json.loads(json_str)
                
                # éªŒè¯æ˜ å°„ç»“æœ
                mapped_fields = set()
                for group_fields in mapping.values():
                    mapped_fields.update(group_fields)
                
                # æ·»åŠ æœªæ˜ å°„çš„å­—æ®µ
                for field in original_fields:
                    if field not in mapped_fields:
                        mapping[field] = [field]
                
                return mapping
        except Exception as e:
            logger.warning(f"è§£æAIå“åº”å¤±è´¥: {e}")
        
        # å¦‚æœè§£æå¤±è´¥ï¼Œè¿”å›é»˜è®¤æ˜ å°„ï¼ˆæ¯ä¸ªå­—æ®µè‡ªæˆä¸€ç»„ï¼‰
        return {field: [field] for field in original_fields}

class ExcelMerger:
    """Excelåˆå¹¶å™¨ä¸»ç±»"""
    
    def __init__(self, deepseek_api_key: str = None):
        self.api = DeepSeekAPI(deepseek_api_key)
        self.field_matcher = FieldMatcher(self.api)
        self.dataframes = []
        self.file_info = []
        self.merged_df = None
        self.merge_report = {}
    
    def load_excel_files(self, file_paths: List[str]) -> bool:
        """åŠ è½½å¤šä¸ªExcelæ–‡ä»¶"""
        logger.info(f"å¼€å§‹åŠ è½½ {len(file_paths)} ä¸ªExcelæ–‡ä»¶")
        
        self.dataframes = []
        self.file_info = []
        
        for file_path in file_paths:
            try:
                if not os.path.exists(file_path):
                    logger.warning(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                    continue
                
                # è¯»å–Excelæ–‡ä»¶
                df = pd.read_excel(file_path)
                
                if df.empty:
                    logger.warning(f"æ–‡ä»¶ä¸ºç©º: {file_path}")
                    continue
                
                self.dataframes.append(df)
                self.file_info.append({
                    'file_path': file_path,
                    'file_name': os.path.basename(file_path),
                    'rows': len(df),
                    'columns': list(df.columns),
                    'column_count': len(df.columns)
                })
                
                logger.info(f"æˆåŠŸåŠ è½½: {os.path.basename(file_path)} ({len(df)}è¡Œ, {len(df.columns)}åˆ—)")
                
            except Exception as e:
                logger.error(f"åŠ è½½æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
        
        logger.info(f"å…±æˆåŠŸåŠ è½½ {len(self.dataframes)} ä¸ªæ–‡ä»¶")
        return len(self.dataframes) > 0
    
    def analyze_and_merge(self) -> bool:
        """åˆ†æå­—æ®µå¹¶åˆå¹¶æ•°æ®"""
        if len(self.dataframes) < 2:
            logger.error("è‡³å°‘éœ€è¦2ä¸ªæ–‡ä»¶æ‰èƒ½è¿›è¡Œåˆå¹¶")
            return False
        
        logger.info("å¼€å§‹åˆ†æå­—æ®µç›¸ä¼¼æ€§...")
        
        # æ”¶é›†æ‰€æœ‰å­—æ®µ
        field_groups = [list(df.columns) for df in self.dataframes]
        
        # AIåˆ†æå­—æ®µç›¸ä¼¼æ€§
        field_mapping = self.field_matcher.analyze_field_similarity(field_groups)
        
        # æ‰§è¡Œåˆå¹¶
        logger.info("å¼€å§‹åˆå¹¶æ•°æ®...")
        merged_df = self._merge_dataframes(field_mapping)
        
        if merged_df is not None:
            self.merged_df = merged_df
            self._generate_merge_report(field_mapping)
            logger.info(f"åˆå¹¶å®Œæˆï¼æœ€ç»ˆæ•°æ®: {len(merged_df)}è¡Œ, {len(merged_df.columns)}åˆ—")
            return True
        
        return False
    
    def _merge_dataframes(self, field_mapping: Dict[str, List[str]]) -> Optional[pd.DataFrame]:
        """æ ¹æ®å­—æ®µæ˜ å°„åˆå¹¶æ•°æ®æ¡†"""
        try:
            # åˆ›å»ºæ ‡å‡†åŒ–çš„åˆ—åæ˜ å°„
            standard_columns = {}
            for standard_name, similar_fields in field_mapping.items():
                for field in similar_fields:
                    standard_columns[field] = standard_name
            
            # æ ‡å‡†åŒ–æ¯ä¸ªæ•°æ®æ¡†çš„åˆ—å
            standardized_dfs = []
            for i, df in enumerate(self.dataframes):
                df_copy = df.copy()
                
                # é‡å‘½ååˆ—
                rename_map = {}
                for col in df_copy.columns:
                    if col in standard_columns:
                        rename_map[col] = standard_columns[col]
                
                df_copy = df_copy.rename(columns=rename_map)
                
                # æ·»åŠ æ•°æ®æºæ ‡è¯†
                df_copy['_source_file'] = self.file_info[i]['file_name']
                df_copy['_source_index'] = i
                
                standardized_dfs.append(df_copy)
            
            # åˆå¹¶æ‰€æœ‰æ•°æ®æ¡†
            merged_df = pd.concat(standardized_dfs, ignore_index=True, sort=False)
            
            # æ•°æ®æ¸…ç†å’Œå»é‡
            merged_df = self._clean_merged_data(merged_df)
            
            return merged_df
            
        except Exception as e:
            logger.error(f"åˆå¹¶æ•°æ®æ¡†å¤±è´¥: {e}")
            return None
    
    def _clean_merged_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ¸…ç†åˆå¹¶åçš„æ•°æ®"""
        logger.info("å¼€å§‹æ•°æ®æ¸…ç†...")
        
        original_rows = len(df)
        
        # ç§»é™¤å®Œå…¨é‡å¤çš„è¡Œï¼ˆé™¤äº†æºæ–‡ä»¶ä¿¡æ¯ï¼‰
        data_columns = [col for col in df.columns if not col.startswith('_source')]
        df_deduplicated = df.drop_duplicates(subset=data_columns, keep='first')
        
        removed_duplicates = original_rows - len(df_deduplicated)
        if removed_duplicates > 0:
            logger.info(f"ç§»é™¤é‡å¤æ•°æ®: {removed_duplicates}è¡Œ")
        
        return df_deduplicated
    
    def _generate_merge_report(self, field_mapping: Dict[str, List[str]]):
        """ç”Ÿæˆåˆå¹¶æŠ¥å‘Š"""
        self.merge_report = {
            'merge_time': datetime.now().isoformat(),
            'source_files': self.file_info,
            'field_mapping': field_mapping,
            'merged_stats': {
                'total_rows': len(self.merged_df),
                'total_columns': len(self.merged_df.columns),
                'data_columns': len([col for col in self.merged_df.columns if not col.startswith('_source')])
            },
            'column_info': {
                col: {
                    'non_null_count': int(self.merged_df[col].count()),
                    'null_count': int(self.merged_df[col].isnull().sum()),
                    'data_type': str(self.merged_df[col].dtype)
                }
                for col in self.merged_df.columns
            }
        }
    
    def save_results(self, output_dir: str = "./output") -> Dict[str, str]:
        """ä¿å­˜åˆå¹¶ç»“æœ"""
        if self.merged_df is None:
            logger.error("æ²¡æœ‰åˆå¹¶ç»“æœå¯ä¿å­˜")
            return {}
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜åˆå¹¶åçš„Excelæ–‡ä»¶
        excel_path = os.path.join(output_dir, f"merged_data_{timestamp}.xlsx")
        self.merged_df.to_excel(excel_path, index=False)
        
        # ä¿å­˜åˆå¹¶æŠ¥å‘Š
        report_path = os.path.join(output_dir, f"merge_report_{timestamp}.json")
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(self.merge_report, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜å­—æ®µæ˜ å°„ä¿¡æ¯
        mapping_path = os.path.join(output_dir, f"field_mapping_{timestamp}.json")
        with open(mapping_path, 'w', encoding='utf-8') as f:
            json.dump(self.merge_report['field_mapping'], f, ensure_ascii=False, indent=2)
        
        result_files = {
            'merged_excel': excel_path,
            'merge_report': report_path,
            'field_mapping': mapping_path
        }
        
        logger.info(f"ç»“æœå·²ä¿å­˜åˆ°: {output_dir}")
        for file_type, file_path in result_files.items():
            logger.info(f"  {file_type}: {file_path}")
        
        return result_files
    
    def print_summary(self):
        """æ‰“å°åˆå¹¶æ‘˜è¦"""
        if not self.merge_report:
            logger.warning("æ²¡æœ‰åˆå¹¶æŠ¥å‘Šå¯æ˜¾ç¤º")
            return
        
        print("\n" + "="*60)
        print("Excelå¤šè¡¨åˆå¹¶æ‘˜è¦æŠ¥å‘Š")
        print("="*60)
        
        print(f"\nğŸ“ æºæ–‡ä»¶ä¿¡æ¯:")
        for i, file_info in enumerate(self.file_info, 1):
            print(f"  {i}. {file_info['file_name']} ({file_info['rows']}è¡Œ, {file_info['column_count']}åˆ—)")
        
        print(f"\nğŸ”— å­—æ®µæ˜ å°„:")
        for standard_name, similar_fields in self.merge_report['field_mapping'].items():
            if len(similar_fields) > 1:
                print(f"  {standard_name} â† {', '.join(similar_fields)}")
        
        stats = self.merge_report['merged_stats']
        print(f"\nğŸ“Š åˆå¹¶ç»“æœ:")
        print(f"  æ€»è¡Œæ•°: {stats['total_rows']}")
        print(f"  æ€»åˆ—æ•°: {stats['total_columns']}")
        print(f"  æ•°æ®åˆ—æ•°: {stats['data_columns']}")
        
        print("\nâœ… åˆå¹¶å®Œæˆï¼")
        print("="*60)

def create_sample_excel_files(output_dir: str = "./sample_data"):
    """åˆ›å»ºç¤ºä¾‹Excelæ–‡ä»¶ç”¨äºæµ‹è¯•"""
    os.makedirs(output_dir, exist_ok=True)
    
    # ç¤ºä¾‹æ•°æ®1 - é”€å”®æŠ¥è¡¨A
    data1 = {
        'è®¢å•ID': ['A001', 'A002', 'A003', 'A004'],
        'å®¢æˆ·åç§°': ['åŒ—äº¬ç§‘æŠ€å…¬å¸', 'ä¸Šæµ·è´¸æ˜“å…¬å¸', 'æ·±åœ³åˆ¶é€ å…¬å¸', 'å¹¿å·æœåŠ¡å…¬å¸'],
        'äº§å“åç§°': ['ç¬”è®°æœ¬ç”µè„‘', 'å°å¼æœº', 'æœåŠ¡å™¨', 'æ‰“å°æœº'],
        'é”€å”®é‡‘é¢': [8500, 6200, 15000, 1200],
        'é”€å”®æ—¥æœŸ': ['2024-01-15', '2024-01-16', '2024-01-17', '2024-01-18']
    }
    
    # ç¤ºä¾‹æ•°æ®2 - é”€å”®æŠ¥è¡¨Bï¼ˆå­—æ®µåä¸åŒï¼‰
    data2 = {
        'ID_è®¢å•': ['B001', 'B002', 'B003'],
        'å®¢æˆ·å…¨ç§°': ['å¤©æ´¥ç”µå­å…¬å¸', 'é‡åº†è½¯ä»¶å…¬å¸', 'æˆéƒ½ç½‘ç»œå…¬å¸'],
        'å•†å“åç§°': ['è·¯ç”±å™¨', 'äº¤æ¢æœº', 'é˜²ç«å¢™'],
        'æˆäº¤é‡‘é¢': [3200, 4500, 8800],
        'æˆäº¤æ—¶é—´': ['2024-01-19', '2024-01-20', '2024-01-21']
    }
    
    # ç¤ºä¾‹æ•°æ®3 - é”€å”®æŠ¥è¡¨Cï¼ˆæ›´å¤šå˜åŒ–ï¼‰
    data3 = {
        'é”€å”®å•å·': ['C001', 'C002'],
        'å…¬å¸åç§°': ['æ­å·åˆ›æ–°å…¬å¸', 'å—äº¬æŠ€æœ¯å…¬å¸'],
        'äº§å“': ['äº‘æœåŠ¡å™¨', 'æ•°æ®åº“è½¯ä»¶'],
        'é‡‘é¢': [12000, 9500],
        'æ—¥æœŸ': ['2024-01-22', '2024-01-23']
    }
    
    # ä¿å­˜ä¸ºExcelæ–‡ä»¶
    pd.DataFrame(data1).to_excel(os.path.join(output_dir, 'é”€å”®æŠ¥è¡¨_åˆ†å…¬å¸A.xlsx'), index=False)
    pd.DataFrame(data2).to_excel(os.path.join(output_dir, 'é”€å”®æŠ¥è¡¨_åˆ†å…¬å¸B.xlsx'), index=False)
    pd.DataFrame(data3).to_excel(os.path.join(output_dir, 'é”€å”®æŠ¥è¡¨_åˆ†å…¬å¸C.xlsx'), index=False)
    
    logger.info(f"ç¤ºä¾‹æ–‡ä»¶å·²åˆ›å»ºåœ¨: {output_dir}")
    return [
        os.path.join(output_dir, 'é”€å”®æŠ¥è¡¨_åˆ†å…¬å¸A.xlsx'),
        os.path.join(output_dir, 'é”€å”®æŠ¥è¡¨_åˆ†å…¬å¸B.xlsx'),
        os.path.join(output_dir, 'é”€å”®æŠ¥è¡¨_åˆ†å…¬å¸C.xlsx')
    ]

def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºç¨‹åºä½¿ç”¨"""
    print("Excelå¤šè¡¨åˆå¹¶å·¥å…· - AIæ™ºèƒ½åŒ¹é…è¡¨å¤´")
    print("ä½¿ç”¨DeepSeek LLMè¯†åˆ«è¯­ä¹‰ç›¸åŒä½†è¡¨è¿°ä¸åŒçš„å­—æ®µ\n")
    
    # åˆ›å»ºç¤ºä¾‹æ–‡ä»¶
    print("1. åˆ›å»ºç¤ºä¾‹Excelæ–‡ä»¶...")
    sample_files = create_sample_excel_files()
    
    # åˆå§‹åŒ–åˆå¹¶å™¨ï¼ˆè¯·æ›¿æ¢ä¸ºå®é™…çš„DeepSeek API Keyï¼‰
    print("2. åˆå§‹åŒ–AIåˆå¹¶å™¨...")
    merger = ExcelMerger(deepseek_api_key="sk-your-deepseek-api-key")
    
    # åŠ è½½æ–‡ä»¶
    print("3. åŠ è½½Excelæ–‡ä»¶...")
    if not merger.load_excel_files(sample_files):
        print("âŒ æ–‡ä»¶åŠ è½½å¤±è´¥")
        return
    
    # åˆ†æå¹¶åˆå¹¶
    print("4. AIåˆ†æå­—æ®µå¹¶åˆå¹¶æ•°æ®...")
    if not merger.analyze_and_merge():
        print("âŒ åˆå¹¶å¤±è´¥")
        return
    
    # ä¿å­˜ç»“æœ
    print("5. ä¿å­˜åˆå¹¶ç»“æœ...")
    result_files = merger.save_results()
    
    # æ˜¾ç¤ºæ‘˜è¦
    merger.print_summary()
    
    print(f"\nğŸ‰ ç¨‹åºæ‰§è¡Œå®Œæˆï¼")
    print(f"ğŸ“ ç»“æœæ–‡ä»¶:")
    for file_type, file_path in result_files.items():
        print(f"  {file_type}: {file_path}")

if __name__ == "__main__":
    main()