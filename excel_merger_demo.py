#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Excelå¤šè¡¨åˆå¹¶å·¥å…· - æ¼”ç¤ºç‰ˆ
ä¸“ä¸ºæ¼”ç¤ºè®¾è®¡ï¼Œæ— éœ€APIå¯†é’¥å³å¯è¿è¡Œï¼Œä¸»è¦ä½¿ç”¨æ¨¡ç³ŠåŒ¹é…ç®—æ³•

åŠŸèƒ½ç‰¹ç‚¹ï¼š
1. æ— éœ€APIå¯†é’¥ï¼Œå¼€ç®±å³ç”¨
2. æ™ºèƒ½æ¨¡ç³ŠåŒ¹é…ç®—æ³•
3. é¢„å®šä¹‰å­—æ®µæ¨¡å¼åŒ¹é…
4. å®Œæ•´çš„åˆå¹¶æŠ¥å‘Š
5. æ”¯æŒæ‰¹é‡å¤„ç†

ä½œè€…ï¼šAI Assistant
æ—¥æœŸï¼š2025-01-29
"""

import os
import json
import pandas as pd
from typing import List, Dict, Tuple, Optional
from pathlib import Path
import logging
from datetime import datetime
import hashlib
from collections import defaultdict
import re
from fuzzywuzzy import fuzz, process
import argparse

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('excel_merger_demo.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class DemoFieldMatcher:
    """æ¼”ç¤ºç‰ˆå­—æ®µåŒ¹é…å™¨ - åŸºäºæ¨¡ç³ŠåŒ¹é…å’Œé¢„å®šä¹‰è§„åˆ™"""
    
    def __init__(self):
        self.similarity_threshold = 0.75
        self.common_patterns = {
            "è®¢å•ID": ["è®¢å•ID", "è®¢å•ç¼–å·", "ID_è®¢å•", "é”€å”®å•å·", "order_id", "order_number", "å•å·"],
            "å®¢æˆ·åç§°": ["å®¢æˆ·åç§°", "å®¢æˆ·å", "å®¢æˆ·å…¨ç§°", "å…¬å¸åç§°", "customer_name", "company_name", "å®¢æˆ·"],
            "äº§å“åç§°": ["äº§å“åç§°", "å•†å“åç§°", "äº§å“", "å•†å“", "product_name", "item_name", "è´§å“"],
            "é‡‘é¢": ["é‡‘é¢", "é”€å”®é‡‘é¢", "æˆäº¤é‡‘é¢", "æ€»é‡‘é¢", "amount", "price", "total", "ä»·æ ¼"],
            "æ—¥æœŸ": ["æ—¥æœŸ", "é”€å”®æ—¥æœŸ", "æˆäº¤æ—¶é—´", "æ—¶é—´", "date", "time", "created_at", "è®¢å•æ—¥æœŸ"],
            "é”€å”®å‘˜": ["é”€å”®å‘˜", "ä¸šåŠ¡å‘˜", "è´Ÿè´£äºº", "salesperson", "sales_rep", "å‘˜å·¥"]
        }
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.match_stats = {
            'pattern_matches': 0,
            'fuzzy_matches': 0,
            'no_matches': 0
        }
    
    def analyze_field_similarity(self, field_groups: List[List[str]]) -> Dict[str, List[str]]:
        """åˆ†æå­—æ®µç›¸ä¼¼æ€§"""
        all_fields = []
        for group in field_groups:
            all_fields.extend(group)
        
        unique_fields = list(set(all_fields))
        
        if len(unique_fields) <= 1:
            return {unique_fields[0]: unique_fields} if unique_fields else {}
        
        logger.info(f"å¼€å§‹åˆ†æ {len(unique_fields)} ä¸ªå­—æ®µçš„ç›¸ä¼¼æ€§...")
        
        # é¦–å…ˆä½¿ç”¨é¢„å®šä¹‰æ¨¡å¼åŒ¹é…
        pattern_mapping = self._match_with_patterns(unique_fields)
        
        # å¯¹æœªåŒ¹é…çš„å­—æ®µä½¿ç”¨æ¨¡ç³ŠåŒ¹é…
        unmatched_fields = [f for f in unique_fields if not any(f in group for group in pattern_mapping.values())]
        fuzzy_mapping = self._fuzzy_match_remaining(unmatched_fields)
        
        # åˆå¹¶ç»“æœ
        final_mapping = {**pattern_mapping, **fuzzy_mapping}
        
        # è®°å½•ç»Ÿè®¡ä¿¡æ¯
        self._log_match_results(final_mapping, unique_fields)
        
        return final_mapping
    
    def _match_with_patterns(self, fields: List[str]) -> Dict[str, List[str]]:
        """ä½¿ç”¨é¢„å®šä¹‰æ¨¡å¼åŒ¹é…å­—æ®µ"""
        mapping = {}
        matched_fields = set()
        
        for standard_name, patterns in self.common_patterns.items():
            matched_group = []
            
            for field in fields:
                if field in matched_fields:
                    continue
                
                # æ£€æŸ¥æ˜¯å¦åŒ¹é…ä»»ä½•æ¨¡å¼
                for pattern in patterns:
                    if self._is_similar(field, pattern, threshold=0.85):
                        matched_group.append(field)
                        matched_fields.add(field)
                        self.match_stats['pattern_matches'] += 1
                        logger.debug(f"æ¨¡å¼åŒ¹é…: {field} -> {standard_name} (æ¨¡å¼: {pattern})")
                        break
            
            if matched_group:
                mapping[standard_name] = matched_group
        
        return mapping
    
    def _fuzzy_match_remaining(self, fields: List[str]) -> Dict[str, List[str]]:
        """å¯¹å‰©ä½™å­—æ®µè¿›è¡Œæ¨¡ç³ŠåŒ¹é…"""
        if not fields:
            return {}
        
        mapping = {}
        processed = set()
        
        for i, field1 in enumerate(fields):
            if field1 in processed:
                continue
            
            similar_group = [field1]
            processed.add(field1)
            
            for j, field2 in enumerate(fields[i+1:], i+1):
                if field2 in processed:
                    continue
                
                if self._is_similar(field1, field2):
                    similar_group.append(field2)
                    processed.add(field2)
                    self.match_stats['fuzzy_matches'] += 1
                    logger.debug(f"æ¨¡ç³ŠåŒ¹é…: {field1} <-> {field2}")
            
            # ä½¿ç”¨æœ€çŸ­çš„å­—æ®µåä½œä¸ºæ ‡å‡†å
            standard_name = min(similar_group, key=len)
            mapping[standard_name] = similar_group
        
        # ç»Ÿè®¡æœªåŒ¹é…çš„å­—æ®µ
        for field in fields:
            if field in processed:
                continue
            self.match_stats['no_matches'] += 1
        
        return mapping
    
    def _is_similar(self, field1: str, field2: str, threshold: float = None) -> bool:
        """åˆ¤æ–­ä¸¤ä¸ªå­—æ®µæ˜¯å¦ç›¸ä¼¼"""
        if threshold is None:
            threshold = self.similarity_threshold
        
        # æ ‡å‡†åŒ–å­—æ®µåï¼ˆå»é™¤ç©ºæ ¼ã€ä¸‹åˆ’çº¿ï¼Œè½¬å°å†™ï¼‰
        norm1 = re.sub(r'[\s_-]', '', field1.lower())
        norm2 = re.sub(r'[\s_-]', '', field2.lower())
        
        # å®Œå…¨åŒ¹é…
        if norm1 == norm2:
            return True
        
        # åŒ…å«å…³ç³»æ£€æŸ¥
        if norm1 in norm2 or norm2 in norm1:
            return True
        
        # æ¨¡ç³ŠåŒ¹é…
        similarity = fuzz.ratio(norm1, norm2) / 100.0
        return similarity >= threshold
    
    def _log_match_results(self, mapping: Dict[str, List[str]], original_fields: List[str]):
        """è®°å½•åŒ¹é…ç»“æœ"""
        logger.info(f"å­—æ®µåŒ¹é…å®Œæˆ:")
        logger.info(f"  åŸå§‹å­—æ®µæ•°: {len(original_fields)}")
        logger.info(f"  æ˜ å°„ç»„æ•°: {len(mapping)}")
        logger.info(f"  æ¨¡å¼åŒ¹é…: {self.match_stats['pattern_matches']}")
        logger.info(f"  æ¨¡ç³ŠåŒ¹é…: {self.match_stats['fuzzy_matches']}")
        logger.info(f"  æœªåŒ¹é…: {self.match_stats['no_matches']}")
        
        # æ˜¾ç¤ºæœ‰å†²çªçš„å­—æ®µç»„
        conflicts = [(k, v) for k, v in mapping.items() if len(v) > 1]
        if conflicts:
            logger.info(f"å‘ç° {len(conflicts)} ä¸ªå­—æ®µç»„æœ‰å¤šä¸ªå˜ä½“:")
            for standard_name, variants in conflicts:
                logger.info(f"  {standard_name}: {', '.join(variants)}")

class ExcelMergerDemo:
    """æ¼”ç¤ºç‰ˆExcelåˆå¹¶å™¨"""
    
    def __init__(self):
        self.field_matcher = DemoFieldMatcher()
        self.dataframes = []
        self.file_info = []
        self.merged_df = None
        self.merge_report = {}
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'start_time': None,
            'end_time': None,
            'processing_time': None,
            'files_processed': 0,
            'files_failed': 0,
            'total_rows_input': 0,
            'total_rows_output': 0,
            'duplicates_removed': 0
        }
    
    def load_excel_files(self, file_paths: List[str]) -> bool:
        """åŠ è½½å¤šä¸ªExcelæ–‡ä»¶"""
        self.stats['start_time'] = datetime.now()
        logger.info(f"å¼€å§‹åŠ è½½ {len(file_paths)} ä¸ªExcelæ–‡ä»¶")
        
        self.dataframes = []
        self.file_info = []
        
        for file_path in file_paths:
            try:
                if not os.path.exists(file_path):
                    logger.warning(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                    self.stats['files_failed'] += 1
                    continue
                
                # è¯»å–Excelæ–‡ä»¶
                df = pd.read_excel(file_path)
                
                if df.empty:
                    logger.warning(f"æ–‡ä»¶ä¸ºç©º: {file_path}")
                    self.stats['files_failed'] += 1
                    continue
                
                self.dataframes.append(df)
                self.file_info.append({
                    'file_path': file_path,
                    'file_name': os.path.basename(file_path),
                    'rows': len(df),
                    'columns': list(df.columns),
                    'column_count': len(df.columns),
                    'file_size': os.path.getsize(file_path),
                    'load_time': datetime.now().isoformat()
                })
                
                self.stats['files_processed'] += 1
                self.stats['total_rows_input'] += len(df)
                
                logger.info(f"âœ… {os.path.basename(file_path)} ({len(df)}è¡Œ, {len(df.columns)}åˆ—)")
                logger.info(f"   å­—æ®µ: {', '.join(df.columns)}")
                
            except Exception as e:
                logger.error(f"âŒ åŠ è½½å¤±è´¥ {file_path}: {e}")
                self.stats['files_failed'] += 1
        
        logger.info(f"\nğŸ“Š åŠ è½½æ±‡æ€»: æˆåŠŸ {self.stats['files_processed']} ä¸ªï¼Œå¤±è´¥ {self.stats['files_failed']} ä¸ª")
        return len(self.dataframes) > 0
    
    def analyze_and_merge(self) -> bool:
        """åˆ†æå­—æ®µå¹¶åˆå¹¶æ•°æ®"""
        if len(self.dataframes) < 2:
            logger.error("âŒ è‡³å°‘éœ€è¦2ä¸ªæ–‡ä»¶æ‰èƒ½è¿›è¡Œåˆå¹¶")
            return False
        
        logger.info("\nğŸ” å¼€å§‹æ™ºèƒ½å­—æ®µåˆ†æ...")
        
        # æ”¶é›†æ‰€æœ‰å­—æ®µ
        field_groups = [list(df.columns) for df in self.dataframes]
        
        # åˆ†æå­—æ®µç›¸ä¼¼æ€§
        field_mapping = self.field_matcher.analyze_field_similarity(field_groups)
        
        # æ‰§è¡Œåˆå¹¶
        logger.info("\nğŸ”„ å¼€å§‹æ•°æ®åˆå¹¶...")
        merged_df = self._merge_dataframes(field_mapping)
        
        if merged_df is not None:
            self.merged_df = merged_df
            self.stats['total_rows_output'] = len(merged_df)
            self.stats['end_time'] = datetime.now()
            self.stats['processing_time'] = (self.stats['end_time'] - self.stats['start_time']).total_seconds()
            
            self._generate_merge_report(field_mapping)
            logger.info(f"âœ… åˆå¹¶å®Œæˆï¼æœ€ç»ˆæ•°æ®: {len(merged_df):,}è¡Œ, {len(merged_df.columns)}åˆ—")
            return True
        
        return False
    
    def _merge_dataframes(self, field_mapping: Dict[str, List[str]]) -> Optional[pd.DataFrame]:
        """æ ¹æ®å­—æ®µæ˜ å°„åˆå¹¶æ•°æ®æ¡†"""
        try:
            # åˆ›å»ºæ ‡å‡†åŒ–çš„åˆ—åæ˜ å°„
            standard_columns = {}
            for standard_name, similar_fields in field_mapping.items():
                for field in similar_fields:
                    standard_columns[field] = standard_name
            
            # æ ‡å‡†åŒ–æ¯ä¸ªæ•°æ®æ¡†çš„åˆ—å
            standardized_dfs = []
            for i, df in enumerate(self.dataframes):
                df_copy = df.copy()
                
                # é‡å‘½ååˆ—
                rename_map = {}
                for col in df_copy.columns:
                    if col in standard_columns:
                        rename_map[col] = standard_columns[col]
                
                if rename_map:
                    df_copy = df_copy.rename(columns=rename_map)
                    logger.info(f"ğŸ“ æ–‡ä»¶ {i+1} å­—æ®µé‡å‘½å: {rename_map}")
                
                # æ·»åŠ æ•°æ®æºæ ‡è¯†
                df_copy['_æ•°æ®æº'] = self.file_info[i]['file_name']
                df_copy['_æºæ–‡ä»¶åºå·'] = i + 1
                
                standardized_dfs.append(df_copy)
            
            # åˆå¹¶æ‰€æœ‰æ•°æ®æ¡†
            merged_df = pd.concat(standardized_dfs, ignore_index=True, sort=False)
            
            # æ•°æ®æ¸…ç†
            merged_df = self._clean_merged_data(merged_df)
            
            return merged_df
            
        except Exception as e:
            logger.error(f"âŒ åˆå¹¶æ•°æ®æ¡†å¤±è´¥: {e}")
            return None
    
    def _clean_merged_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ¸…ç†åˆå¹¶åçš„æ•°æ®"""
        logger.info("ğŸ§¹ å¼€å§‹æ•°æ®æ¸…ç†...")
        
        original_rows = len(df)
        
        # ç§»é™¤å®Œå…¨é‡å¤çš„è¡Œï¼ˆé™¤äº†æºæ–‡ä»¶ä¿¡æ¯ï¼‰
        data_columns = [col for col in df.columns if not col.startswith('_')]
        if data_columns:
            df_deduplicated = df.drop_duplicates(subset=data_columns, keep='first')
        else:
            df_deduplicated = df
        
        removed_duplicates = original_rows - len(df_deduplicated)
        self.stats['duplicates_removed'] = removed_duplicates
        
        if removed_duplicates > 0:
            logger.info(f"ğŸ—‘ï¸  ç§»é™¤é‡å¤æ•°æ®: {removed_duplicates}è¡Œ")
        else:
            logger.info("âœ¨ æœªå‘ç°é‡å¤æ•°æ®")
        
        return df_deduplicated
    
    def _generate_merge_report(self, field_mapping: Dict[str, List[str]]):
        """ç”Ÿæˆè¯¦ç»†çš„åˆå¹¶æŠ¥å‘Š"""
        self.merge_report = {
            'merge_info': {
                'merge_time': datetime.now().isoformat(),
                'processing_time_seconds': round(self.stats['processing_time'], 2),
                'tool_version': 'Demo 1.0.0',
                'merge_method': 'Fuzzy Matching + Pattern Recognition'
            },
            'source_files': self.file_info,
            'field_mapping': field_mapping,
            'statistics': {
                'input_stats': {
                    'files_processed': self.stats['files_processed'],
                    'files_failed': self.stats['files_failed'],
                    'total_input_rows': self.stats['total_rows_input']
                },
                'output_stats': {
                    'total_output_rows': self.stats['total_rows_output'],
                    'duplicates_removed': self.stats['duplicates_removed'],
                    'data_columns': len([col for col in self.merged_df.columns if not col.startswith('_')]),
                    'total_columns': len(self.merged_df.columns)
                },
                'matching_stats': self.field_matcher.match_stats
            },
            'column_analysis': {
                col: {
                    'non_null_count': int(self.merged_df[col].count()),
                    'null_count': int(self.merged_df[col].isnull().sum()),
                    'null_percentage': round(float(self.merged_df[col].isnull().sum() / len(self.merged_df) * 100), 2),
                    'data_type': str(self.merged_df[col].dtype),
                    'unique_values': int(self.merged_df[col].nunique()) if not col.startswith('_') else None
                }
                for col in self.merged_df.columns
            },
            'field_mapping_details': {
                'total_unique_fields': len(set(field for group in field_mapping.values() for field in group)),
                'mapped_groups': len(field_mapping),
                'fields_with_conflicts': len([k for k, v in field_mapping.items() if len(v) > 1])
            }
        }
    
    def save_results(self, output_dir: str = "./output") -> Dict[str, str]:
        """ä¿å­˜åˆå¹¶ç»“æœ"""
        if self.merged_df is None:
            logger.error("âŒ æ²¡æœ‰åˆå¹¶ç»“æœå¯ä¿å­˜")
            return {}
        
        os.makedirs(output_dir, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        result_files = {}
        
        # ä¿å­˜åˆå¹¶åçš„Excelæ–‡ä»¶
        excel_path = os.path.join(output_dir, f"åˆå¹¶ç»“æœ_{timestamp}.xlsx")
        self.merged_df.to_excel(excel_path, index=False)
        result_files['merged_data'] = excel_path
        
        # ä¿å­˜åˆå¹¶æŠ¥å‘Š
        report_path = os.path.join(output_dir, f"åˆå¹¶æŠ¥å‘Š_{timestamp}.json")
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(self.merge_report, f, ensure_ascii=False, indent=2)
        result_files['merge_report'] = report_path
        
        # ä¿å­˜å­—æ®µæ˜ å°„ä¿¡æ¯
        mapping_path = os.path.join(output_dir, f"å­—æ®µæ˜ å°„_{timestamp}.json")
        with open(mapping_path, 'w', encoding='utf-8') as f:
            json.dump(self.merge_report['field_mapping'], f, ensure_ascii=False, indent=2)
        result_files['field_mapping'] = mapping_path
        
        logger.info(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_dir}")
        for file_type, file_path in result_files.items():
            logger.info(f"   ğŸ“„ {os.path.basename(file_path)}")
        
        return result_files
    
    def print_summary(self):
        """æ‰“å°è¯¦ç»†çš„åˆå¹¶æ‘˜è¦"""
        if not self.merge_report:
            logger.warning("âš ï¸  æ²¡æœ‰åˆå¹¶æŠ¥å‘Šå¯æ˜¾ç¤º")
            return
        
        print("\n" + "="*80)
        print("ğŸ¯ Excelå¤šè¡¨åˆå¹¶æ‘˜è¦æŠ¥å‘Š - æ¼”ç¤ºç‰ˆ")
        print("="*80)
        
        # å¤„ç†ç»Ÿè®¡
        stats = self.merge_report['statistics']
        print(f"\nâ±ï¸  å¤„ç†ç»Ÿè®¡:")
        print(f"   å¤„ç†æ—¶é—´: {self.stats['processing_time']:.2f}ç§’")
        print(f"   æˆåŠŸæ–‡ä»¶: {stats['input_stats']['files_processed']}ä¸ª")
        print(f"   å¤±è´¥æ–‡ä»¶: {stats['input_stats']['files_failed']}ä¸ª")
        
        # åŒ¹é…ç»Ÿè®¡
        match_stats = stats['matching_stats']
        print(f"\nğŸ¯ å­—æ®µåŒ¹é…ç»Ÿè®¡:")
        print(f"   æ¨¡å¼åŒ¹é…: {match_stats['pattern_matches']}æ¬¡")
        print(f"   æ¨¡ç³ŠåŒ¹é…: {match_stats['fuzzy_matches']}æ¬¡")
        print(f"   æœªåŒ¹é…: {match_stats['no_matches']}æ¬¡")
        
        # æºæ–‡ä»¶ä¿¡æ¯
        print(f"\nğŸ“ æºæ–‡ä»¶ä¿¡æ¯:")
        for i, file_info in enumerate(self.file_info, 1):
            size_mb = file_info['file_size'] / 1024 / 1024
            print(f"   {i}. {file_info['file_name']}")
            print(f"      ğŸ“Š {file_info['rows']:,}è¡Œ Ã— {file_info['column_count']}åˆ— ({size_mb:.1f}MB)")
            print(f"      ğŸ“ å­—æ®µ: {', '.join(file_info['columns'])}")
        
        # å­—æ®µæ˜ å°„
        print(f"\nğŸ”— æ™ºèƒ½å­—æ®µæ˜ å°„:")
        mapping_details = self.merge_report['field_mapping_details']
        print(f"   åŸå§‹å­—æ®µæ•°: {mapping_details['total_unique_fields']}")
        print(f"   æ˜ å°„ç»„æ•°: {mapping_details['mapped_groups']}")
        print(f"   æœ‰å˜ä½“çš„å­—æ®µç»„: {mapping_details['fields_with_conflicts']}")
        
        print(f"\n   ğŸ“‹ å…·ä½“æ˜ å°„å…³ç³»:")
        for standard_name, similar_fields in self.merge_report['field_mapping'].items():
            if len(similar_fields) > 1:
                print(f"      ğŸ¯ {standard_name} â† [{', '.join(similar_fields)}]")
            else:
                print(f"      ğŸ“Œ {standard_name}")
        
        # åˆå¹¶ç»“æœ
        output_stats = stats['output_stats']
        print(f"\nğŸ“Š åˆå¹¶ç»“æœ:")
        print(f"   è¾“å…¥æ€»è¡Œæ•°: {stats['input_stats']['total_input_rows']:,}")
        print(f"   è¾“å‡ºæ€»è¡Œæ•°: {output_stats['total_output_rows']:,}")
        print(f"   ç§»é™¤é‡å¤: {output_stats['duplicates_removed']:,}è¡Œ")
        print(f"   æ•°æ®åˆ—æ•°: {output_stats['data_columns']}")
        print(f"   æ€»åˆ—æ•°: {output_stats['total_columns']}")
        
        # æ•°æ®è´¨é‡é¢„è§ˆ
        print(f"\nğŸ“ˆ æ•°æ®è´¨é‡é¢„è§ˆ:")
        data_columns = [col for col in self.merged_df.columns if not col.startswith('_')]
        for col in data_columns[:5]:  # åªæ˜¾ç¤ºå‰5åˆ—
            col_info = self.merge_report['column_analysis'][col]
            completeness = 100 - col_info['null_percentage']
            print(f"   ğŸ“Š {col}: {col_info['non_null_count']:,}éç©ºå€¼ ({completeness:.1f}%å®Œæ•´åº¦)")
        
        if len(data_columns) > 5:
            print(f"   ... è¿˜æœ‰ {len(data_columns)-5} åˆ—")
        
        print("\nâœ… åˆå¹¶å®Œæˆï¼æ•°æ®å·²å‡†å¤‡å°±ç»ª")
        print("="*80)

def create_demo_sample_files(output_dir: str = "./demo_sample_data"):
    """åˆ›å»ºæ¼”ç¤ºç”¨çš„ç¤ºä¾‹Excelæ–‡ä»¶"""
    os.makedirs(output_dir, exist_ok=True)
    
    # ç¤ºä¾‹æ•°æ®1 - æ€»éƒ¨é”€å”®æŠ¥è¡¨
    data1 = {
        'è®¢å•ID': ['HQ001', 'HQ002', 'HQ003', 'HQ004', 'HQ005'],
        'å®¢æˆ·åç§°': ['åŒ—äº¬ç§‘æŠ€æœ‰é™å…¬å¸', 'ä¸Šæµ·è´¸æ˜“é›†å›¢æœ‰é™å…¬å¸', 'æ·±åœ³åˆ¶é€ ä¼ä¸š', 'å¹¿å·æœåŠ¡å…¬å¸', 'æ­å·åˆ›æ–°ç§‘æŠ€'],
        'äº§å“åç§°': ['ç¬”è®°æœ¬ç”µè„‘', 'å°å¼æœº', 'æœåŠ¡å™¨', 'æ‰“å°æœº', 'è·¯ç”±å™¨'],
        'é”€å”®é‡‘é¢': [8500, 6200, 15000, 1200, 3200],
        'é”€å”®æ—¥æœŸ': ['2024-01-15', '2024-01-16', '2024-01-17', '2024-01-18', '2024-01-19'],
        'é”€å”®å‘˜': ['å¼ ä¸‰', 'æå››', 'ç‹äº”', 'èµµå…­', 'é’±ä¸ƒ']
    }
    
    # ç¤ºä¾‹æ•°æ®2 - ååŒ—åˆ†å…¬å¸ï¼ˆå­—æ®µåç•¥æœ‰ä¸åŒï¼‰
    data2 = {
        'ID_è®¢å•': ['BJ001', 'BJ002', 'BJ003', 'BJ004'],
        'å®¢æˆ·å…¨ç§°': ['å¤©æ´¥ç”µå­ç§‘æŠ€å…¬å¸', 'çŸ³å®¶åº„è½¯ä»¶å¼€å‘å…¬å¸', 'å¤ªåŸç½‘ç»œæŠ€æœ¯å…¬å¸', 'å‘¼å’Œæµ©ç‰¹é€šä¿¡è®¾å¤‡å…¬å¸'],
        'å•†å“åç§°': ['äº¤æ¢æœº', 'é˜²ç«å¢™', 'å­˜å‚¨è®¾å¤‡', 'ç›‘æ§ç³»ç»Ÿ'],
        'æˆäº¤é‡‘é¢': [4500, 8800, 12000, 6500],
        'æˆäº¤æ—¶é—´': ['2024-01-20', '2024-01-21', '2024-01-22', '2024-01-23'],
        'ä¸šåŠ¡å‘˜': ['å­™å…«', 'å‘¨ä¹', 'å´å', 'éƒ‘ä¸€']
    }
    
    # ç¤ºä¾‹æ•°æ®3 - åä¸œåˆ†å…¬å¸ï¼ˆå­—æ®µåå·®å¼‚æ›´å¤§ï¼‰
    data3 = {
        'é”€å”®å•å·': ['SH001', 'SH002', 'SH003'],
        'å…¬å¸åç§°': ['å—äº¬æŠ€æœ¯æœ‰é™å…¬å¸', 'è‹å·åˆ¶é€ é›†å›¢', 'æ— é”¡è´¸æ˜“å…¬å¸'],
        'äº§å“': ['äº‘æœåŠ¡å™¨', 'æ•°æ®åº“è½¯ä»¶', 'åŠå…¬è½¯ä»¶'],
        'é‡‘é¢': [18000, 9500, 3500],
        'æ—¥æœŸ': ['2024-01-24', '2024-01-25', '2024-01-26'],
        'è´Ÿè´£äºº': ['é™ˆäºŒ', 'æ—ä¸‰', 'é»„å››']
    }
    
    # ç¤ºä¾‹æ•°æ®4 - åå—åˆ†å…¬å¸ï¼ˆåŒ…å«é‡å¤æ•°æ®å’Œè‹±æ–‡å­—æ®µï¼‰
    data4 = {
        'order_id': ['HQ001', 'GZ001', 'GZ002'],  # HQ001æ˜¯é‡å¤æ•°æ®
        'customer_name': ['åŒ—äº¬ç§‘æŠ€æœ‰é™å…¬å¸', 'æ·±åœ³æµ·æ´‹å…¬å¸', 'ç æµ·æ¸¯å£å…¬å¸'],
        'product_name': ['ç¬”è®°æœ¬ç”µè„‘', 'èˆ¹èˆ¶è®¾å¤‡', 'æ¸¯å£æœºæ¢°'],
        'amount': [8500, 25000, 18000],
        'sale_date': ['2024-01-15', '2024-01-27', '2024-01-28'],
        'salesperson': ['å¼ ä¸‰', 'åˆ˜äº”', 'é™ˆå…­']
    }
    
    # ç¤ºä¾‹æ•°æ®5 - è¥¿å—åˆ†å…¬å¸ï¼ˆæ›´å¤šå­—æ®µå˜ä½“ï¼‰
    data5 = {
        'å•å·': ['CD001', 'CD002'],
        'å®¢æˆ·': ['æˆéƒ½è½¯ä»¶å…¬å¸', 'é‡åº†åˆ¶é€ ä¼ä¸š'],
        'è´§å“': ['ERPç³»ç»Ÿ', 'ç”Ÿäº§è®¾å¤‡'],
        'ä»·æ ¼': [45000, 32000],
        'è®¢å•æ—¥æœŸ': ['2024-01-29', '2024-01-30'],
        'å‘˜å·¥': ['ç‹ä¸ƒ', 'æå…«']
    }
    
    # ä¿å­˜ä¸ºExcelæ–‡ä»¶
    files = [
        ('é”€å”®æŠ¥è¡¨_æ€»éƒ¨.xlsx', data1),
        ('é”€å”®æŠ¥è¡¨_ååŒ—åˆ†å…¬å¸.xlsx', data2),
        ('é”€å”®æŠ¥è¡¨_åä¸œåˆ†å…¬å¸.xlsx', data3),
        ('é”€å”®æŠ¥è¡¨_åå—åˆ†å…¬å¸.xlsx', data4),
        ('é”€å”®æŠ¥è¡¨_è¥¿å—åˆ†å…¬å¸.xlsx', data5)
    ]
    
    file_paths = []
    for filename, data in files:
        file_path = os.path.join(output_dir, filename)
        pd.DataFrame(data).to_excel(file_path, index=False)
        file_paths.append(file_path)
    
    logger.info(f"ğŸ“ æ¼”ç¤ºç¤ºä¾‹æ–‡ä»¶å·²åˆ›å»ºåœ¨: {output_dir}")
    return file_paths

def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºç‰ˆ"""
    parser = argparse.ArgumentParser(description='Excelå¤šè¡¨åˆå¹¶å·¥å…· - æ¼”ç¤ºç‰ˆï¼ˆæ— éœ€APIå¯†é’¥ï¼‰')
    parser.add_argument('--files', nargs='+', help='è¦åˆå¹¶çš„Excelæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output', default='./demo_output', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--demo', action='store_true', help='è¿è¡Œå®Œæ•´æ¼”ç¤º')
    parser.add_argument('--create-sample', action='store_true', help='ä»…åˆ›å»ºç¤ºä¾‹æ–‡ä»¶')
    
    args = parser.parse_args()
    
    print("ğŸš€ Excelå¤šè¡¨åˆå¹¶å·¥å…· - æ¼”ç¤ºç‰ˆ")
    print("ğŸ’¡ åŸºäºæ™ºèƒ½æ¨¡ç³ŠåŒ¹é…ç®—æ³•ï¼Œæ— éœ€APIå¯†é’¥å³å¯ä½¿ç”¨")
    print("ğŸ¯ è‡ªåŠ¨è¯†åˆ«è¯­ä¹‰ç›¸åŒä½†è¡¨è¿°ä¸åŒçš„å­—æ®µå\n")
    
    # ä»…åˆ›å»ºç¤ºä¾‹æ–‡ä»¶
    if args.create_sample:
        print("ğŸ“ åˆ›å»ºæ¼”ç¤ºç¤ºä¾‹æ–‡ä»¶...")
        sample_files = create_demo_sample_files()
        print(f"âœ… ç¤ºä¾‹æ–‡ä»¶å·²åˆ›å»º: {len(sample_files)}ä¸ª")
        for file_path in sample_files:
            print(f"   ğŸ“„ {os.path.basename(file_path)}")
        return
    
    # å®Œæ•´æ¼”ç¤ºæ¨¡å¼
    if args.demo:
        print("ğŸ¬ 1. åˆ›å»ºæ¼”ç¤ºç¤ºä¾‹æ–‡ä»¶...")
        sample_files = create_demo_sample_files()
        file_paths = sample_files
        print(f"âœ… å·²åˆ›å»º {len(sample_files)} ä¸ªç¤ºä¾‹æ–‡ä»¶")
    else:
        if not args.files:
            print("âŒ é”™è¯¯: è¯·æŒ‡å®šè¦åˆå¹¶çš„Excelæ–‡ä»¶è·¯å¾„")
            print("ğŸ’¡ ä½¿ç”¨ --demo è¿è¡Œæ¼”ç¤ºï¼Œæˆ– --help æŸ¥çœ‹å¸®åŠ©")
            return
        file_paths = args.files
    
    # åˆå§‹åŒ–åˆå¹¶å™¨
    print("\nğŸ¤– 2. åˆå§‹åŒ–æ™ºèƒ½åˆå¹¶å™¨...")
    merger = ExcelMergerDemo()
    
    # åŠ è½½æ–‡ä»¶
    print("\nğŸ“‚ 3. åŠ è½½Excelæ–‡ä»¶...")
    if not merger.load_excel_files(file_paths):
        print("âŒ æ–‡ä»¶åŠ è½½å¤±è´¥")
        return
    
    # åˆ†æå¹¶åˆå¹¶
    print("\nğŸ§  4. æ™ºèƒ½åˆ†æå­—æ®µå¹¶åˆå¹¶æ•°æ®...")
    if not merger.analyze_and_merge():
        print("âŒ åˆå¹¶å¤±è´¥")
        return
    
    # ä¿å­˜ç»“æœ
    print("\nğŸ’¾ 5. ä¿å­˜åˆå¹¶ç»“æœ...")
    result_files = merger.save_results(args.output)
    
    # æ˜¾ç¤ºæ‘˜è¦
    merger.print_summary()
    
    print(f"\nğŸ‰ æ¼”ç¤ºå®Œæˆï¼")
    print(f"ğŸ“ ç»“æœæ–‡ä»¶ä½ç½®: {args.output}")
    for file_type, file_path in result_files.items():
        print(f"   ğŸ“„ {os.path.basename(file_path)}")
    
    print(f"\nğŸ’¡ æç¤º: æ‚¨å¯ä»¥æ‰“å¼€Excelæ–‡ä»¶æŸ¥çœ‹åˆå¹¶ç»“æœ")
    print(f"ğŸ“Š åˆå¹¶æŠ¥å‘ŠåŒ…å«è¯¦ç»†çš„å­—æ®µæ˜ å°„å’Œç»Ÿè®¡ä¿¡æ¯")

if __name__ == "__main__":
    main()