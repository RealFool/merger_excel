#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Excelå¤šè¡¨åˆå¹¶å·¥å…· - å¢å¼ºç‰ˆ
ä½¿ç”¨DeepSeek LLM + æ¨¡ç³ŠåŒ¹é…åŒé‡ç­–ç•¥è¯†åˆ«è¯­ä¹‰ç›¸åŒä½†è¡¨è¿°ä¸åŒçš„å­—æ®µ

æ–°å¢åŠŸèƒ½ï¼š
1. é…ç½®æ–‡ä»¶æ”¯æŒ
2. æ¨¡ç³ŠåŒ¹é…å¤‡é€‰æ–¹æ¡ˆ
3. æ‰¹é‡å¤„ç†æ¨¡å¼
4. æ›´è¯¦ç»†çš„åˆå¹¶æŠ¥å‘Š
5. é”™è¯¯æ¢å¤æœºåˆ¶
6. å­—æ®µæ˜ å°„ç¼“å­˜

ä½œè€…ï¼šAI Assistant
æ—¥æœŸï¼š2025-01-29
"""

import os
import json
import pandas as pd
import requests
from typing import List, Dict, Tuple, Optional, Union
from pathlib import Path
import logging
from datetime import datetime
import hashlib
from collections import defaultdict
import re
from difflib import SequenceMatcher
from fuzzywuzzy import fuzz, process
import argparse
import sys

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('excel_merger_enhanced.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class ConfigManager:
    """é…ç½®ç®¡ç†å™¨"""
    
    def __init__(self, config_path: str = "config_excel_merger.json"):
        self.config_path = config_path
        self.config = self.load_config()
    
    def load_config(self) -> Dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            if os.path.exists(self.config_path):
                with open(self.config_path, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                logger.info(f"é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ: {self.config_path}")
                return config
            else:
                logger.warning(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {self.config_path}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
                return self.get_default_config()
        except Exception as e:
            logger.error(f"é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            return self.get_default_config()
    
    def get_default_config(self) -> Dict:
        """è·å–é»˜è®¤é…ç½®"""
        return {
            "deepseek_api": {
                "api_key": "sk-your-deepseek-api-key",
                "base_url": "https://api.deepseek.com",
                "model": "deepseek-chat",
                "temperature": 0.1,
                "max_tokens": 2000,
                "timeout": 30
            },
            "merge_settings": {
                "remove_duplicates": True,
                "add_source_info": True,
                "case_sensitive": False,
                "auto_detect_encoding": True
            },
            "output_settings": {
                "output_dir": "./output",
                "save_report": True,
                "save_field_mapping": True,
                "excel_format": "xlsx"
            },
            "field_matching": {
                "similarity_threshold": 0.8,
                "use_ai_analysis": True,
                "fallback_to_fuzzy_match": True,
                "common_field_patterns": {
                    "order_id": ["è®¢å•ID", "è®¢å•ç¼–å·", "ID_è®¢å•", "é”€å”®å•å·"],
                    "customer_name": ["å®¢æˆ·åç§°", "å®¢æˆ·å", "å®¢æˆ·å…¨ç§°", "å…¬å¸åç§°"],
                    "product_name": ["äº§å“åç§°", "å•†å“åç§°", "äº§å“", "å•†å“"],
                    "amount": ["é‡‘é¢", "é”€å”®é‡‘é¢", "æˆäº¤é‡‘é¢", "æ€»é‡‘é¢"],
                    "date": ["æ—¥æœŸ", "é”€å”®æ—¥æœŸ", "æˆäº¤æ—¶é—´", "æ—¶é—´"]
                }
            }
        }
    
    def get(self, key_path: str, default=None):
        """è·å–é…ç½®å€¼ï¼Œæ”¯æŒç‚¹å·åˆ†éš”çš„è·¯å¾„"""
        keys = key_path.split('.')
        value = self.config
        
        for key in keys:
            if isinstance(value, dict) and key in value:
                value = value[key]
            else:
                return default
        
        return value

class DeepSeekAPIEnhanced:
    """å¢å¼ºç‰ˆDeepSeek APIè°ƒç”¨ç±»"""
    
    def __init__(self, config_manager: ConfigManager):
        self.config = config_manager
        self.api_key = self.config.get('deepseek_api.api_key')
        self.base_url = self.config.get('deepseek_api.base_url')
        self.model = self.config.get('deepseek_api.model')
        self.temperature = self.config.get('deepseek_api.temperature')
        self.max_tokens = self.config.get('deepseek_api.max_tokens')
        self.timeout = self.config.get('deepseek_api.timeout')
        
        self.headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        # APIè°ƒç”¨ç»Ÿè®¡
        self.api_calls = 0
        self.api_errors = 0
    
    def call_api(self, messages: List[Dict]) -> str:
        """è°ƒç”¨DeepSeek API"""
        try:
            self.api_calls += 1
            
            payload = {
                "model": self.model,
                "messages": messages,
                "temperature": self.temperature,
                "max_tokens": self.max_tokens
            }
            
            response = requests.post(
                f"{self.base_url}/v1/chat/completions",
                headers=self.headers,
                json=payload,
                timeout=self.timeout
            )
            
            if response.status_code == 200:
                result = response.json()
                return result['choices'][0]['message']['content']
            else:
                self.api_errors += 1
                logger.error(f"APIè°ƒç”¨å¤±è´¥: {response.status_code} - {response.text}")
                return ""
                
        except Exception as e:
            self.api_errors += 1
            logger.error(f"APIè°ƒç”¨å¼‚å¸¸: {e}")
            return ""
    
    def get_stats(self) -> Dict:
        """è·å–APIè°ƒç”¨ç»Ÿè®¡"""
        return {
            'total_calls': self.api_calls,
            'errors': self.api_errors,
            'success_rate': (self.api_calls - self.api_errors) / max(self.api_calls, 1)
        }

class FuzzyFieldMatcher:
    """æ¨¡ç³Šå­—æ®µåŒ¹é…å™¨"""
    
    def __init__(self, config_manager: ConfigManager):
        self.config = config_manager
        self.similarity_threshold = self.config.get('field_matching.similarity_threshold', 0.8)
        self.common_patterns = self.config.get('field_matching.common_field_patterns', {})
    
    def fuzzy_match_fields(self, field_groups: List[List[str]]) -> Dict[str, List[str]]:
        """ä½¿ç”¨æ¨¡ç³ŠåŒ¹é…ç®—æ³•åŒ¹é…å­—æ®µ"""
        all_fields = []
        for group in field_groups:
            all_fields.extend(group)
        
        unique_fields = list(set(all_fields))
        
        if len(unique_fields) <= 1:
            return {unique_fields[0]: unique_fields} if unique_fields else {}
        
        # é¦–å…ˆå°è¯•ä½¿ç”¨é¢„å®šä¹‰æ¨¡å¼åŒ¹é…
        pattern_mapping = self._match_with_patterns(unique_fields)
        
        # å¯¹æœªåŒ¹é…çš„å­—æ®µä½¿ç”¨æ¨¡ç³ŠåŒ¹é…
        unmatched_fields = [f for f in unique_fields if not any(f in group for group in pattern_mapping.values())]
        fuzzy_mapping = self._fuzzy_match_remaining(unmatched_fields)
        
        # åˆå¹¶ç»“æœ
        final_mapping = {**pattern_mapping, **fuzzy_mapping}
        
        logger.info(f"æ¨¡ç³ŠåŒ¹é…ç»“æœ: {final_mapping}")
        return final_mapping
    
    def _match_with_patterns(self, fields: List[str]) -> Dict[str, List[str]]:
        """ä½¿ç”¨é¢„å®šä¹‰æ¨¡å¼åŒ¹é…å­—æ®µ"""
        mapping = {}
        matched_fields = set()
        
        for standard_name, patterns in self.common_patterns.items():
            matched_group = []
            
            for field in fields:
                if field in matched_fields:
                    continue
                
                # æ£€æŸ¥æ˜¯å¦åŒ¹é…ä»»ä½•æ¨¡å¼
                for pattern in patterns:
                    if self._is_similar(field, pattern):
                        matched_group.append(field)
                        matched_fields.add(field)
                        break
            
            if matched_group:
                mapping[standard_name] = matched_group
        
        return mapping
    
    def _fuzzy_match_remaining(self, fields: List[str]) -> Dict[str, List[str]]:
        """å¯¹å‰©ä½™å­—æ®µè¿›è¡Œæ¨¡ç³ŠåŒ¹é…"""
        if not fields:
            return {}
        
        mapping = {}
        processed = set()
        
        for i, field1 in enumerate(fields):
            if field1 in processed:
                continue
            
            similar_group = [field1]
            processed.add(field1)
            
            for j, field2 in enumerate(fields[i+1:], i+1):
                if field2 in processed:
                    continue
                
                if self._is_similar(field1, field2):
                    similar_group.append(field2)
                    processed.add(field2)
            
            # ä½¿ç”¨æœ€çŸ­çš„å­—æ®µåä½œä¸ºæ ‡å‡†å
            standard_name = min(similar_group, key=len)
            mapping[standard_name] = similar_group
        
        return mapping
    
    def _is_similar(self, field1: str, field2: str) -> bool:
        """åˆ¤æ–­ä¸¤ä¸ªå­—æ®µæ˜¯å¦ç›¸ä¼¼"""
        # æ ‡å‡†åŒ–å­—æ®µåï¼ˆå»é™¤ç©ºæ ¼ã€ä¸‹åˆ’çº¿ï¼Œè½¬å°å†™ï¼‰
        norm1 = re.sub(r'[\s_-]', '', field1.lower())
        norm2 = re.sub(r'[\s_-]', '', field2.lower())
        
        # å®Œå…¨åŒ¹é…
        if norm1 == norm2:
            return True
        
        # æ¨¡ç³ŠåŒ¹é…
        similarity = fuzz.ratio(norm1, norm2) / 100.0
        return similarity >= self.similarity_threshold

class EnhancedFieldMatcher:
    """å¢å¼ºç‰ˆå­—æ®µåŒ¹é…å™¨ - ç»“åˆAIå’Œæ¨¡ç³ŠåŒ¹é…"""
    
    def __init__(self, deepseek_api: DeepSeekAPIEnhanced, config_manager: ConfigManager):
        self.api = deepseek_api
        self.config = config_manager
        self.fuzzy_matcher = FuzzyFieldMatcher(config_manager)
        self.field_mapping_cache = {}
        self.use_ai = self.config.get('field_matching.use_ai_analysis', True)
        self.fallback_fuzzy = self.config.get('field_matching.fallback_to_fuzzy_match', True)
    
    def analyze_field_similarity(self, field_groups: List[List[str]]) -> Dict[str, List[str]]:
        """åˆ†æå­—æ®µç›¸ä¼¼æ€§ï¼Œä¼˜å…ˆä½¿ç”¨AIï¼Œå¤±è´¥æ—¶å›é€€åˆ°æ¨¡ç³ŠåŒ¹é…"""
        
        # ç”Ÿæˆç¼“å­˜é”®
        cache_key = self._generate_cache_key(field_groups)
        if cache_key in self.field_mapping_cache:
            logger.info("ä½¿ç”¨ç¼“å­˜çš„å­—æ®µæ˜ å°„ç»“æœ")
            return self.field_mapping_cache[cache_key]
        
        # æ„å»ºæ‰€æœ‰å­—æ®µåˆ—è¡¨
        all_fields = []
        for group in field_groups:
            all_fields.extend(group)
        
        unique_fields = list(set(all_fields))
        
        if len(unique_fields) <= 1:
            result = {unique_fields[0]: unique_fields} if unique_fields else {}
            self.field_mapping_cache[cache_key] = result
            return result
        
        # å°è¯•AIåˆ†æ
        if self.use_ai:
            logger.info("ä½¿ç”¨AIåˆ†æå­—æ®µç›¸ä¼¼æ€§...")
            ai_result = self._ai_analyze_fields(unique_fields)
            
            if ai_result:
                logger.info("AIåˆ†ææˆåŠŸ")
                self.field_mapping_cache[cache_key] = ai_result
                return ai_result
            else:
                logger.warning("AIåˆ†æå¤±è´¥")
        
        # å›é€€åˆ°æ¨¡ç³ŠåŒ¹é…
        if self.fallback_fuzzy:
            logger.info("ä½¿ç”¨æ¨¡ç³ŠåŒ¹é…ä½œä¸ºå¤‡é€‰æ–¹æ¡ˆ...")
            fuzzy_result = self.fuzzy_matcher.fuzzy_match_fields(field_groups)
            self.field_mapping_cache[cache_key] = fuzzy_result
            return fuzzy_result
        
        # æœ€åçš„å¤‡é€‰æ–¹æ¡ˆï¼šæ¯ä¸ªå­—æ®µè‡ªæˆä¸€ç»„
        logger.warning("æ‰€æœ‰åŒ¹é…æ–¹æ³•éƒ½å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤æ˜ å°„")
        default_result = {field: [field] for field in unique_fields}
        self.field_mapping_cache[cache_key] = default_result
        return default_result
    
    def _generate_cache_key(self, field_groups: List[List[str]]) -> str:
        """ç”Ÿæˆå­—æ®µç»„çš„ç¼“å­˜é”®"""
        all_fields = sorted(set(field for group in field_groups for field in group))
        return hashlib.md5('|'.join(all_fields).encode()).hexdigest()
    
    def _ai_analyze_fields(self, fields: List[str]) -> Optional[Dict[str, List[str]]]:
        """ä½¿ç”¨AIåˆ†æå­—æ®µ"""
        try:
            prompt = self._build_field_analysis_prompt(fields)
            
            messages = [
                {
                    "role": "system",
                    "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ•°æ®åˆ†æå¸ˆï¼Œæ“…é•¿è¯†åˆ«è¡¨æ ¼ä¸­è¯­ä¹‰ç›¸åŒä½†è¡¨è¿°ä¸åŒçš„å­—æ®µåã€‚è¯·ä»”ç»†åˆ†æå­—æ®µåçš„è¯­ä¹‰å«ä¹‰ï¼Œå°†ç›¸åŒå«ä¹‰çš„å­—æ®µå½’ä¸ºä¸€ç»„ã€‚"
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ]
            
            response = self.api.call_api(messages)
            
            if response:
                return self._parse_field_mapping_response(response, fields)
            
        except Exception as e:
            logger.error(f"AIå­—æ®µåˆ†æå¼‚å¸¸: {e}")
        
        return None
    
    def _build_field_analysis_prompt(self, fields: List[str]) -> str:
        """æ„å»ºå­—æ®µåˆ†ææç¤ºè¯"""
        fields_text = "\n".join([f"{i+1}. {field}" for i, field in enumerate(fields)])
        
        prompt = f"""
è¯·åˆ†æä»¥ä¸‹å­—æ®µåï¼Œè¯†åˆ«è¯­ä¹‰ç›¸åŒä½†è¡¨è¿°ä¸åŒçš„å­—æ®µï¼Œå¹¶å°†å®ƒä»¬å½’ä¸ºä¸€ç»„ã€‚

å­—æ®µåˆ—è¡¨ï¼š
{fields_text}

åˆ†æè¦æ±‚ï¼š
1. è¯†åˆ«è¡¨ç¤ºç›¸åŒä¸šåŠ¡å«ä¹‰çš„å­—æ®µï¼ˆå¦‚ï¼šè®¢å•IDã€ID_è®¢å•ã€è®¢å•ç¼–å·ã€é”€å”®å•å·éƒ½è¡¨ç¤ºè®¢å•æ ‡è¯†ï¼‰
2. è€ƒè™‘å¸¸è§çš„ä¸šåŠ¡å­—æ®µï¼šå®¢æˆ·ä¿¡æ¯ã€è®¢å•ä¿¡æ¯ã€äº§å“ä¿¡æ¯ã€é‡‘é¢ã€æ—¶é—´ç­‰
3. å¿½ç•¥å¤§å°å†™ã€ä¸‹åˆ’çº¿ã€ç©ºæ ¼ç­‰æ ¼å¼å·®å¼‚
4. è€ƒè™‘ä¸­è‹±æ–‡æ··ç”¨ã€ç®€å†™ã€å…¨ç§°ç­‰æƒ…å†µ
5. ç›¸ä¼¼åº¦è¾ƒä½çš„å­—æ®µåº”è¯¥åˆ†å¼€å½’ç»„

è¯·ä»¥JSONæ ¼å¼è¿”å›ç»“æœï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{{
  "group_1": ["å­—æ®µ1", "å­—æ®µ2", "å­—æ®µ3"],
  "group_2": ["å­—æ®µ4", "å­—æ®µ5"],
  "group_3": ["å­—æ®µ6"]
}}

å…¶ä¸­æ¯ä¸ªgroupä»£è¡¨ä¸€ç»„è¯­ä¹‰ç›¸åŒçš„å­—æ®µï¼Œgroupçš„å‘½åè¯·ä½¿ç”¨æœ€é€šç”¨çš„å­—æ®µåã€‚
ç¡®ä¿æ¯ä¸ªè¾“å…¥å­—æ®µéƒ½è¢«åˆ†é…åˆ°æŸä¸ªç»„ä¸­ã€‚
"""
        return prompt
    
    def _parse_field_mapping_response(self, response: str, original_fields: List[str]) -> Optional[Dict[str, List[str]]]:
        """è§£æAIå“åº”ï¼Œæå–å­—æ®µæ˜ å°„å…³ç³»"""
        try:
            # å°è¯•æå–JSON
            json_match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', response, re.DOTALL)
            if json_match:
                json_str = json_match.group()
                mapping = json.loads(json_str)
                
                # éªŒè¯æ˜ å°„ç»“æœ
                mapped_fields = set()
                for group_fields in mapping.values():
                    if isinstance(group_fields, list):
                        mapped_fields.update(group_fields)
                
                # æ·»åŠ æœªæ˜ å°„çš„å­—æ®µ
                for field in original_fields:
                    if field not in mapped_fields:
                        mapping[field] = [field]
                
                return mapping
        except Exception as e:
            logger.warning(f"è§£æAIå“åº”å¤±è´¥: {e}")
        
        return None

class ExcelMergerEnhanced:
    """å¢å¼ºç‰ˆExcelåˆå¹¶å™¨"""
    
    def __init__(self, config_path: str = "config_excel_merger.json"):
        self.config_manager = ConfigManager(config_path)
        self.api = DeepSeekAPIEnhanced(self.config_manager)
        self.field_matcher = EnhancedFieldMatcher(self.api, self.config_manager)
        
        self.dataframes = []
        self.file_info = []
        self.merged_df = None
        self.merge_report = {}
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'start_time': None,
            'end_time': None,
            'processing_time': None,
            'files_processed': 0,
            'files_failed': 0,
            'total_rows_input': 0,
            'total_rows_output': 0,
            'duplicates_removed': 0
        }
    
    def load_excel_files(self, file_paths: List[str]) -> bool:
        """åŠ è½½å¤šä¸ªExcelæ–‡ä»¶"""
        self.stats['start_time'] = datetime.now()
        logger.info(f"å¼€å§‹åŠ è½½ {len(file_paths)} ä¸ªExcelæ–‡ä»¶")
        
        self.dataframes = []
        self.file_info = []
        
        for file_path in file_paths:
            try:
                if not os.path.exists(file_path):
                    logger.warning(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                    self.stats['files_failed'] += 1
                    continue
                
                # è¯»å–Excelæ–‡ä»¶
                df = pd.read_excel(file_path)
                
                if df.empty:
                    logger.warning(f"æ–‡ä»¶ä¸ºç©º: {file_path}")
                    self.stats['files_failed'] += 1
                    continue
                
                self.dataframes.append(df)
                self.file_info.append({
                    'file_path': file_path,
                    'file_name': os.path.basename(file_path),
                    'rows': len(df),
                    'columns': list(df.columns),
                    'column_count': len(df.columns),
                    'file_size': os.path.getsize(file_path),
                    'load_time': datetime.now().isoformat()
                })
                
                self.stats['files_processed'] += 1
                self.stats['total_rows_input'] += len(df)
                
                logger.info(f"æˆåŠŸåŠ è½½: {os.path.basename(file_path)} ({len(df)}è¡Œ, {len(df.columns)}åˆ—)")
                
            except Exception as e:
                logger.error(f"åŠ è½½æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
                self.stats['files_failed'] += 1
        
        logger.info(f"å…±æˆåŠŸåŠ è½½ {len(self.dataframes)} ä¸ªæ–‡ä»¶")
        return len(self.dataframes) > 0
    
    def analyze_and_merge(self) -> bool:
        """åˆ†æå­—æ®µå¹¶åˆå¹¶æ•°æ®"""
        if len(self.dataframes) < 2:
            logger.error("è‡³å°‘éœ€è¦2ä¸ªæ–‡ä»¶æ‰èƒ½è¿›è¡Œåˆå¹¶")
            return False
        
        logger.info("å¼€å§‹åˆ†æå­—æ®µç›¸ä¼¼æ€§...")
        
        # æ”¶é›†æ‰€æœ‰å­—æ®µ
        field_groups = [list(df.columns) for df in self.dataframes]
        
        # åˆ†æå­—æ®µç›¸ä¼¼æ€§
        field_mapping = self.field_matcher.analyze_field_similarity(field_groups)
        
        # æ‰§è¡Œåˆå¹¶
        logger.info("å¼€å§‹åˆå¹¶æ•°æ®...")
        merged_df = self._merge_dataframes(field_mapping)
        
        if merged_df is not None:
            self.merged_df = merged_df
            self.stats['total_rows_output'] = len(merged_df)
            self.stats['end_time'] = datetime.now()
            self.stats['processing_time'] = (self.stats['end_time'] - self.stats['start_time']).total_seconds()
            
            self._generate_merge_report(field_mapping)
            logger.info(f"åˆå¹¶å®Œæˆï¼æœ€ç»ˆæ•°æ®: {len(merged_df)}è¡Œ, {len(merged_df.columns)}åˆ—")
            return True
        
        return False
    
    def _merge_dataframes(self, field_mapping: Dict[str, List[str]]) -> Optional[pd.DataFrame]:
        """æ ¹æ®å­—æ®µæ˜ å°„åˆå¹¶æ•°æ®æ¡†"""
        try:
            # åˆ›å»ºæ ‡å‡†åŒ–çš„åˆ—åæ˜ å°„
            standard_columns = {}
            for standard_name, similar_fields in field_mapping.items():
                for field in similar_fields:
                    standard_columns[field] = standard_name
            
            # æ ‡å‡†åŒ–æ¯ä¸ªæ•°æ®æ¡†çš„åˆ—å
            standardized_dfs = []
            for i, df in enumerate(self.dataframes):
                df_copy = df.copy()
                
                # é‡å‘½ååˆ—
                rename_map = {}
                for col in df_copy.columns:
                    if col in standard_columns:
                        rename_map[col] = standard_columns[col]
                
                df_copy = df_copy.rename(columns=rename_map)
                
                # æ·»åŠ æ•°æ®æºæ ‡è¯†
                if self.config_manager.get('merge_settings.add_source_info', True):
                    df_copy['_source_file'] = self.file_info[i]['file_name']
                    df_copy['_source_index'] = i
                    df_copy['_row_index'] = df_copy.index
                
                standardized_dfs.append(df_copy)
            
            # åˆå¹¶æ‰€æœ‰æ•°æ®æ¡†
            merged_df = pd.concat(standardized_dfs, ignore_index=True, sort=False)
            
            # æ•°æ®æ¸…ç†
            if self.config_manager.get('merge_settings.remove_duplicates', True):
                merged_df = self._clean_merged_data(merged_df)
            
            return merged_df
            
        except Exception as e:
            logger.error(f"åˆå¹¶æ•°æ®æ¡†å¤±è´¥: {e}")
            return None
    
    def _clean_merged_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ¸…ç†åˆå¹¶åçš„æ•°æ®"""
        logger.info("å¼€å§‹æ•°æ®æ¸…ç†...")
        
        original_rows = len(df)
        
        # ç§»é™¤å®Œå…¨é‡å¤çš„è¡Œï¼ˆé™¤äº†æºæ–‡ä»¶ä¿¡æ¯ï¼‰
        data_columns = [col for col in df.columns if not col.startswith('_source') and col != '_row_index']
        if data_columns:
            df_deduplicated = df.drop_duplicates(subset=data_columns, keep='first')
        else:
            df_deduplicated = df
        
        removed_duplicates = original_rows - len(df_deduplicated)
        self.stats['duplicates_removed'] = removed_duplicates
        
        if removed_duplicates > 0:
            logger.info(f"ç§»é™¤é‡å¤æ•°æ®: {removed_duplicates}è¡Œ")
        
        return df_deduplicated
    
    def _generate_merge_report(self, field_mapping: Dict[str, List[str]]):
        """ç”Ÿæˆè¯¦ç»†çš„åˆå¹¶æŠ¥å‘Š"""
        api_stats = self.api.get_stats()
        
        self.merge_report = {
            'merge_info': {
                'merge_time': datetime.now().isoformat(),
                'processing_time_seconds': self.stats['processing_time'],
                'tool_version': '2.0.0',
                'config_file': self.config_manager.config_path
            },
            'source_files': self.file_info,
            'field_mapping': field_mapping,
            'statistics': {
                'input_stats': {
                    'files_processed': self.stats['files_processed'],
                    'files_failed': self.stats['files_failed'],
                    'total_input_rows': self.stats['total_rows_input']
                },
                'output_stats': {
                    'total_output_rows': self.stats['total_rows_output'],
                    'duplicates_removed': self.stats['duplicates_removed'],
                    'data_columns': len([col for col in self.merged_df.columns if not col.startswith('_')]),
                    'total_columns': len(self.merged_df.columns)
                },
                'api_stats': api_stats
            },
            'column_analysis': {
                col: {
                    'non_null_count': int(self.merged_df[col].count()),
                    'null_count': int(self.merged_df[col].isnull().sum()),
                    'null_percentage': float(self.merged_df[col].isnull().sum() / len(self.merged_df) * 100),
                    'data_type': str(self.merged_df[col].dtype),
                    'unique_values': int(self.merged_df[col].nunique()) if not col.startswith('_') else None
                }
                for col in self.merged_df.columns
            },
            'field_mapping_details': {
                'total_unique_fields': len(set(field for group in field_mapping.values() for field in group)),
                'mapped_groups': len(field_mapping),
                'fields_with_conflicts': len([k for k, v in field_mapping.items() if len(v) > 1])
            }
        }
    
    def save_results(self, output_dir: str = None) -> Dict[str, str]:
        """ä¿å­˜åˆå¹¶ç»“æœ"""
        if self.merged_df is None:
            logger.error("æ²¡æœ‰åˆå¹¶ç»“æœå¯ä¿å­˜")
            return {}
        
        # ä½¿ç”¨é…ç½®ä¸­çš„è¾“å‡ºç›®å½•
        if output_dir is None:
            output_dir = self.config_manager.get('output_settings.output_dir', './output')
        
        os.makedirs(output_dir, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        result_files = {}
        
        # ä¿å­˜åˆå¹¶åçš„Excelæ–‡ä»¶
        excel_format = self.config_manager.get('output_settings.excel_format', 'xlsx')
        excel_path = os.path.join(output_dir, f"merged_data_{timestamp}.{excel_format}")
        
        if excel_format == 'xlsx':
            self.merged_df.to_excel(excel_path, index=False)
        else:
            self.merged_df.to_csv(excel_path, index=False, encoding='utf-8-sig')
        
        result_files['merged_data'] = excel_path
        
        # ä¿å­˜åˆå¹¶æŠ¥å‘Š
        if self.config_manager.get('output_settings.save_report', True):
            report_path = os.path.join(output_dir, f"merge_report_{timestamp}.json")
            with open(report_path, 'w', encoding='utf-8') as f:
                json.dump(self.merge_report, f, ensure_ascii=False, indent=2)
            result_files['merge_report'] = report_path
        
        # ä¿å­˜å­—æ®µæ˜ å°„ä¿¡æ¯
        if self.config_manager.get('output_settings.save_field_mapping', True):
            mapping_path = os.path.join(output_dir, f"field_mapping_{timestamp}.json")
            with open(mapping_path, 'w', encoding='utf-8') as f:
                json.dump(self.merge_report['field_mapping'], f, ensure_ascii=False, indent=2)
            result_files['field_mapping'] = mapping_path
        
        logger.info(f"ç»“æœå·²ä¿å­˜åˆ°: {output_dir}")
        for file_type, file_path in result_files.items():
            logger.info(f"  {file_type}: {file_path}")
        
        return result_files
    
    def print_summary(self):
        """æ‰“å°è¯¦ç»†çš„åˆå¹¶æ‘˜è¦"""
        if not self.merge_report:
            logger.warning("æ²¡æœ‰åˆå¹¶æŠ¥å‘Šå¯æ˜¾ç¤º")
            return
        
        print("\n" + "="*80)
        print("Excelå¤šè¡¨åˆå¹¶æ‘˜è¦æŠ¥å‘Š - å¢å¼ºç‰ˆ")
        print("="*80)
        
        # å¤„ç†ç»Ÿè®¡
        stats = self.merge_report['statistics']
        print(f"\nâ±ï¸  å¤„ç†ç»Ÿè®¡:")
        print(f"  å¤„ç†æ—¶é—´: {self.stats['processing_time']:.2f}ç§’")
        print(f"  æˆåŠŸæ–‡ä»¶: {stats['input_stats']['files_processed']}")
        print(f"  å¤±è´¥æ–‡ä»¶: {stats['input_stats']['files_failed']}")
        print(f"  APIè°ƒç”¨: {stats['api_stats']['total_calls']}æ¬¡ (æˆåŠŸç‡: {stats['api_stats']['success_rate']:.1%})")
        
        # æºæ–‡ä»¶ä¿¡æ¯
        print(f"\nğŸ“ æºæ–‡ä»¶ä¿¡æ¯:")
        for i, file_info in enumerate(self.file_info, 1):
            size_mb = file_info['file_size'] / 1024 / 1024
            print(f"  {i}. {file_info['file_name']} ({file_info['rows']}è¡Œ, {file_info['column_count']}åˆ—, {size_mb:.1f}MB)")
        
        # å­—æ®µæ˜ å°„
        print(f"\nğŸ”— å­—æ®µæ˜ å°„:")
        mapping_details = self.merge_report['field_mapping_details']
        print(f"  åŸå§‹å­—æ®µæ•°: {mapping_details['total_unique_fields']}")
        print(f"  æ˜ å°„ç»„æ•°: {mapping_details['mapped_groups']}")
        print(f"  æœ‰å†²çªçš„å­—æ®µç»„: {mapping_details['fields_with_conflicts']}")
        
        for standard_name, similar_fields in self.merge_report['field_mapping'].items():
            if len(similar_fields) > 1:
                print(f"    {standard_name} â† {', '.join(similar_fields)}")
        
        # åˆå¹¶ç»“æœ
        output_stats = stats['output_stats']
        print(f"\nğŸ“Š åˆå¹¶ç»“æœ:")
        print(f"  è¾“å…¥æ€»è¡Œæ•°: {stats['input_stats']['total_input_rows']:,}")
        print(f"  è¾“å‡ºæ€»è¡Œæ•°: {output_stats['total_output_rows']:,}")
        print(f"  ç§»é™¤é‡å¤: {output_stats['duplicates_removed']:,}è¡Œ")
        print(f"  æ•°æ®åˆ—æ•°: {output_stats['data_columns']}")
        print(f"  æ€»åˆ—æ•°: {output_stats['total_columns']}")
        
        # æ•°æ®è´¨é‡
        print(f"\nğŸ“ˆ æ•°æ®è´¨é‡:")
        data_columns = [col for col in self.merged_df.columns if not col.startswith('_')]
        for col in data_columns[:5]:  # åªæ˜¾ç¤ºå‰5åˆ—
            col_info = self.merge_report['column_analysis'][col]
            print(f"  {col}: {col_info['non_null_count']:,}éç©º ({100-col_info['null_percentage']:.1f}%)")
        
        if len(data_columns) > 5:
            print(f"  ... è¿˜æœ‰ {len(data_columns)-5} åˆ—")
        
        print("\nâœ… åˆå¹¶å®Œæˆï¼")
        print("="*80)

def create_enhanced_sample_files(output_dir: str = "./sample_data_enhanced"):
    """åˆ›å»ºå¢å¼ºç‰ˆç¤ºä¾‹Excelæ–‡ä»¶"""
    os.makedirs(output_dir, exist_ok=True)
    
    # ç¤ºä¾‹æ•°æ®1 - é”€å”®æŠ¥è¡¨A
    data1 = {
        'è®¢å•ID': ['A001', 'A002', 'A003', 'A004', 'A005'],
        'å®¢æˆ·åç§°': ['åŒ—äº¬ç§‘æŠ€æœ‰é™å…¬å¸', 'ä¸Šæµ·è´¸æ˜“é›†å›¢', 'æ·±åœ³åˆ¶é€ ä¼ä¸š', 'å¹¿å·æœåŠ¡å…¬å¸', 'æ­å·åˆ›æ–°ç§‘æŠ€'],
        'äº§å“åç§°': ['ç¬”è®°æœ¬ç”µè„‘', 'å°å¼æœº', 'æœåŠ¡å™¨', 'æ‰“å°æœº', 'è·¯ç”±å™¨'],
        'é”€å”®é‡‘é¢': [8500, 6200, 15000, 1200, 3200],
        'é”€å”®æ—¥æœŸ': ['2024-01-15', '2024-01-16', '2024-01-17', '2024-01-18', '2024-01-19'],
        'é”€å”®å‘˜': ['å¼ ä¸‰', 'æå››', 'ç‹äº”', 'èµµå…­', 'é’±ä¸ƒ']
    }
    
    # ç¤ºä¾‹æ•°æ®2 - é”€å”®æŠ¥è¡¨Bï¼ˆå­—æ®µåä¸åŒï¼‰
    data2 = {
        'ID_è®¢å•': ['B001', 'B002', 'B003', 'B004'],
        'å®¢æˆ·å…¨ç§°': ['å¤©æ´¥ç”µå­ç§‘æŠ€å…¬å¸', 'é‡åº†è½¯ä»¶å¼€å‘å…¬å¸', 'æˆéƒ½ç½‘ç»œæŠ€æœ¯å…¬å¸', 'è¥¿å®‰é€šä¿¡è®¾å¤‡å…¬å¸'],
        'å•†å“åç§°': ['äº¤æ¢æœº', 'é˜²ç«å¢™', 'å­˜å‚¨è®¾å¤‡', 'ç›‘æ§ç³»ç»Ÿ'],
        'æˆäº¤é‡‘é¢': [4500, 8800, 12000, 6500],
        'æˆäº¤æ—¶é—´': ['2024-01-20', '2024-01-21', '2024-01-22', '2024-01-23'],
        'ä¸šåŠ¡å‘˜': ['å­™å…«', 'å‘¨ä¹', 'å´å', 'éƒ‘ä¸€']
    }
    
    # ç¤ºä¾‹æ•°æ®3 - é”€å”®æŠ¥è¡¨Cï¼ˆæ›´å¤šå˜åŒ–ï¼‰
    data3 = {
        'é”€å”®å•å·': ['C001', 'C002', 'C003'],
        'å…¬å¸åç§°': ['å—äº¬æŠ€æœ¯æœ‰é™å…¬å¸', 'è‹å·åˆ¶é€ é›†å›¢', 'æ— é”¡è´¸æ˜“å…¬å¸'],
        'äº§å“': ['äº‘æœåŠ¡å™¨', 'æ•°æ®åº“è½¯ä»¶', 'åŠå…¬è½¯ä»¶'],
        'é‡‘é¢': [18000, 9500, 3500],
        'æ—¥æœŸ': ['2024-01-24', '2024-01-25', '2024-01-26'],
        'è´Ÿè´£äºº': ['é™ˆäºŒ', 'æ—ä¸‰', 'é»„å››']
    }
    
    # ç¤ºä¾‹æ•°æ®4 - åŒ…å«é‡å¤æ•°æ®
    data4 = {
        'order_id': ['A001', 'D001', 'D002'],  # A001æ˜¯é‡å¤æ•°æ®
        'customer_name': ['åŒ—äº¬ç§‘æŠ€æœ‰é™å…¬å¸', 'é’å²›æµ·æ´‹å…¬å¸', 'çƒŸå°æ¸¯å£å…¬å¸'],
        'product_name': ['ç¬”è®°æœ¬ç”µè„‘', 'èˆ¹èˆ¶è®¾å¤‡', 'æ¸¯å£æœºæ¢°'],
        'amount': [8500, 25000, 18000],
        'sale_date': ['2024-01-15', '2024-01-27', '2024-01-28'],
        'salesperson': ['å¼ ä¸‰', 'åˆ˜äº”', 'é™ˆå…­']
    }
    
    # ä¿å­˜ä¸ºExcelæ–‡ä»¶
    files = [
        ('é”€å”®æŠ¥è¡¨_æ€»éƒ¨.xlsx', data1),
        ('é”€å”®æŠ¥è¡¨_ååŒ—åˆ†å…¬å¸.xlsx', data2),
        ('é”€å”®æŠ¥è¡¨_åä¸œåˆ†å…¬å¸.xlsx', data3),
        ('é”€å”®æŠ¥è¡¨_åå—åˆ†å…¬å¸.xlsx', data4)
    ]
    
    file_paths = []
    for filename, data in files:
        file_path = os.path.join(output_dir, filename)
        pd.DataFrame(data).to_excel(file_path, index=False)
        file_paths.append(file_path)
    
    logger.info(f"å¢å¼ºç‰ˆç¤ºä¾‹æ–‡ä»¶å·²åˆ›å»ºåœ¨: {output_dir}")
    return file_paths

def main():
    """ä¸»å‡½æ•° - æ”¯æŒå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='Excelå¤šè¡¨åˆå¹¶å·¥å…· - AIæ™ºèƒ½åŒ¹é…è¡¨å¤´')
    parser.add_argument('--files', nargs='+', help='è¦åˆå¹¶çš„Excelæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--config', default='config_excel_merger.json', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--demo', action='store_true', help='è¿è¡Œæ¼”ç¤ºæ¨¡å¼')
    parser.add_argument('--create-sample', action='store_true', help='åˆ›å»ºç¤ºä¾‹æ–‡ä»¶')
    
    args = parser.parse_args()
    
    print("Excelå¤šè¡¨åˆå¹¶å·¥å…· - AIæ™ºèƒ½åŒ¹é…è¡¨å¤´ (å¢å¼ºç‰ˆ)")
    print("ä½¿ç”¨DeepSeek LLM + æ¨¡ç³ŠåŒ¹é…è¯†åˆ«è¯­ä¹‰ç›¸åŒä½†è¡¨è¿°ä¸åŒçš„å­—æ®µ\n")
    
    # åˆ›å»ºç¤ºä¾‹æ–‡ä»¶
    if args.create_sample:
        print("åˆ›å»ºç¤ºä¾‹Excelæ–‡ä»¶...")
        sample_files = create_enhanced_sample_files()
        print(f"ç¤ºä¾‹æ–‡ä»¶å·²åˆ›å»º: {sample_files}")
        return
    
    # æ¼”ç¤ºæ¨¡å¼
    if args.demo:
        print("1. åˆ›å»ºç¤ºä¾‹Excelæ–‡ä»¶...")
        sample_files = create_enhanced_sample_files()
        file_paths = sample_files
    else:
        if not args.files:
            print("é”™è¯¯: è¯·æŒ‡å®šè¦åˆå¹¶çš„Excelæ–‡ä»¶è·¯å¾„")
            print("ä½¿ç”¨ --help æŸ¥çœ‹å¸®åŠ©ä¿¡æ¯")
            return
        file_paths = args.files
    
    # åˆå§‹åŒ–åˆå¹¶å™¨
    print("2. åˆå§‹åŒ–AIåˆå¹¶å™¨...")
    merger = ExcelMergerEnhanced(args.config)
    
    # åŠ è½½æ–‡ä»¶
    print("3. åŠ è½½Excelæ–‡ä»¶...")
    if not merger.load_excel_files(file_paths):
        print("âŒ æ–‡ä»¶åŠ è½½å¤±è´¥")
        return
    
    # åˆ†æå¹¶åˆå¹¶
    print("4. AIåˆ†æå­—æ®µå¹¶åˆå¹¶æ•°æ®...")
    if not merger.analyze_and_merge():
        print("âŒ åˆå¹¶å¤±è´¥")
        return
    
    # ä¿å­˜ç»“æœ
    print("5. ä¿å­˜åˆå¹¶ç»“æœ...")
    result_files = merger.save_results(args.output)
    
    # æ˜¾ç¤ºæ‘˜è¦
    merger.print_summary()
    
    print(f"\nğŸ‰ ç¨‹åºæ‰§è¡Œå®Œæˆï¼")
    print(f"ğŸ“ ç»“æœæ–‡ä»¶:")
    for file_type, file_path in result_files.items():
        print(f"  {file_type}: {file_path}")

if __name__ == "__main__":
    main()